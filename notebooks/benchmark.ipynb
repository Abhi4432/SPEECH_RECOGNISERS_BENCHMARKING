{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "f86c12ea502841c18c67cca90f227e6d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_f4b7f618b9c04946acc00a43de9b4cc6",
              "IPY_MODEL_154d77a930de4eec83eb68e283a7d101",
              "IPY_MODEL_6905bb2a97dc4fcfb5be75a7f889ea58"
            ],
            "layout": "IPY_MODEL_08111013e9ea481caa4bd947a6e74da8"
          }
        },
        "f4b7f618b9c04946acc00a43de9b4cc6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0ec9a7df8078460daf24c77c5ff76ed6",
            "placeholder": "​",
            "style": "IPY_MODEL_fb503ec2a7b24d86b4dbdf386f912296",
            "value": "parakeet-tdt_ctc-110m.nemo: 100%"
          }
        },
        "154d77a930de4eec83eb68e283a7d101": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_294baaee8fbf4259a98141145b17ac4a",
            "max": 459243520,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_79c5512508a648dcb20e715745af74e4",
            "value": 459243520
          }
        },
        "6905bb2a97dc4fcfb5be75a7f889ea58": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4dc1d33750494c099cea5c19737e1a62",
            "placeholder": "​",
            "style": "IPY_MODEL_2a9315d6899348bba92a41ef98db1d34",
            "value": " 459M/459M [00:14&lt;00:00, 40.5MB/s]"
          }
        },
        "08111013e9ea481caa4bd947a6e74da8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0ec9a7df8078460daf24c77c5ff76ed6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fb503ec2a7b24d86b4dbdf386f912296": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "294baaee8fbf4259a98141145b17ac4a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "79c5512508a648dcb20e715745af74e4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "4dc1d33750494c099cea5c19737e1a62": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2a9315d6899348bba92a41ef98db1d34": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "9UczcSekXGkr"
      },
      "outputs": [],
      "source": [
        "\n",
        "import sys\n",
        "\n",
        "# Add the notebooks folder (where audio_generator.py lives) to Python's module search path\n",
        "sys.path.append(\"/content/drive/MyDrive/speech-recognition-benchmark/notebooks\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7VjI1Bo4YVYi",
        "outputId": "00fdf60f-89bf-4d9c-a7b7-f111fee824a3"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/drive/MyDrive/speech-recognition-benchmark"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R0K5wHGteLyI",
        "outputId": "121426fa-4af6-4227-9266-adb90131559c"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/speech-recognition-benchmark\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -r requirements.txt\n",
        "!apt-get update -qq && apt-get install -y -qq ffmpeg\n"
      ],
      "metadata": {
        "id": "EU9G1jcXeQYB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import time\n",
        "import whisper\n",
        "from jiwer import wer\n",
        "import nemo.collections.asr as nemo_asr\n",
        "from audio_generator import text_to_audio\n",
        "import soundfile as sf\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AkqBeaRdd18K",
        "outputId": "e35ca16a-b153-433d-951f-375e3de96b67"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[NeMo W 2025-09-14 04:27:38 nemo_logging:405] /usr/local/lib/python3.12/dist-packages/pydub/utils.py:300: SyntaxWarning: invalid escape sequence '\\('\n",
            "      m = re.match('([su]([0-9]{1,2})p?) \\(([0-9]{1,2}) bit\\)$', token)\n",
            "    \n",
            "[NeMo W 2025-09-14 04:27:38 nemo_logging:405] /usr/local/lib/python3.12/dist-packages/pydub/utils.py:301: SyntaxWarning: invalid escape sequence '\\('\n",
            "      m2 = re.match('([su]([0-9]{1,2})p?)( \\(default\\))?$', token)\n",
            "    \n",
            "[NeMo W 2025-09-14 04:27:38 nemo_logging:405] /usr/local/lib/python3.12/dist-packages/pydub/utils.py:310: SyntaxWarning: invalid escape sequence '\\('\n",
            "      elif re.match('(flt)p?( \\(default\\))?$', token):\n",
            "    \n",
            "[NeMo W 2025-09-14 04:27:38 nemo_logging:405] /usr/local/lib/python3.12/dist-packages/pydub/utils.py:314: SyntaxWarning: invalid escape sequence '\\('\n",
            "      elif re.match('(dbl)p?( \\(default\\))?$', token):\n",
            "    \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "audio_path = \"audio/test1.wav\"\n",
        "ground_truth = '''Once upon a time, in a quiet village nestled between green hills and winding streams,\n",
        "there lived a curious weaver named Marner. He would wake before dawn to tend to his loom,\n",
        "listening to the soft rhythms of the shuttle and the distant songs of the morning birds.\n",
        "Each day he would think about the stories he had heard as a child — tales of travelers,\n",
        "storms weathered, and friendships that bound the villagers together. On this particular day,\n",
        "Marner decided to walk to the market to see what news had arrived from the neighboring town.\n",
        "He had no idea that a short conversation would change the way he viewed his work and his life.'''"
      ],
      "metadata": {
        "id": "opDtQ-kedTND"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "audio, sr = text_to_audio(ground_truth)\n",
        "\n",
        "# Save manually\n",
        "out_path = audio_path\n",
        "sf.write(out_path, audio, sr)\n",
        "print(\"Saved:\", out_path)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B2KvvyTDc-HU",
        "outputId": "4954cf5f-3908-4e8a-ddfd-43c69cae781a"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved: audio/test1.wav\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def benchmark_whisper(audio_path: str, ground_truth: str, model_size=\"small\"):\n",
        "    \"\"\"\n",
        "    Benchmark Whisper ASR model on one audio file.\n",
        "\n",
        "    Args:\n",
        "        audio_path (str): path to .wav file\n",
        "        ground_truth (str): reference transcript\n",
        "        model_size (str): whisper model size (tiny, base, small, medium, large)\n",
        "                          default = \"small\"\n",
        "\n",
        "    Returns:\n",
        "        dict with {model, latency, transcript, WER}\n",
        "    \"\"\"\n",
        "    # Load model (downloads if not cached)\n",
        "    model = whisper.load_model(model_size)\n",
        "\n",
        "    # Run inference with timing\n",
        "    start = time.time()\n",
        "    result = model.transcribe(audio_path)\n",
        "    end = time.time()\n",
        "\n",
        "    latency = end - start\n",
        "    transcript = result[\"text\"].strip()\n",
        "    error = wer(ground_truth.lower(), transcript.lower())\n",
        "\n",
        "    return {\n",
        "        \"model\": f\"whisper-{model_size}\",\n",
        "        \"latency_sec\": round(latency, 2),\n",
        "        \"transcript\": transcript,\n",
        "        \"WER\": round(error, 3)\n",
        "    }\n"
      ],
      "metadata": {
        "id": "-M0E0YxOdUqH"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "res = benchmark_whisper(audio_path, ground_truth)\n",
        "print(pd.DataFrame([res]))\n",
        "print(res['transcript'])\n"
      ],
      "metadata": {
        "id": "4TWyhAQGdyk0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 135
        },
        "outputId": "657eb860-c538-4bdc-f4b6-7aae293ee6ea"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/whisper/transcribe.py:132: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
            "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "           model  latency_sec  \\\n",
              "0  whisper-small        41.54   \n",
              "\n",
              "                                          transcript    WER  \n",
              "0  Once upon a time, in a quiet village nestled b...  0.017  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-b40a871f-9508-4476-96a0-0880b0475d41\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>model</th>\n",
              "      <th>latency_sec</th>\n",
              "      <th>transcript</th>\n",
              "      <th>WER</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>whisper-small</td>\n",
              "      <td>41.54</td>\n",
              "      <td>Once upon a time, in a quiet village nestled b...</td>\n",
              "      <td>0.017</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-b40a871f-9508-4476-96a0-0880b0475d41')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-b40a871f-9508-4476-96a0-0880b0475d41 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-b40a871f-9508-4476-96a0-0880b0475d41');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "summary": "{\n  \"name\": \"pd\",\n  \"rows\": 1,\n  \"fields\": [\n    {\n      \"column\": \"model\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 1,\n        \"samples\": [\n          \"whisper-small\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"latency_sec\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": null,\n        \"min\": 41.54,\n        \"max\": 41.54,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          41.54\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"transcript\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 1,\n        \"samples\": [\n          \"Once upon a time, in a quiet village nestled between green hills and winding streams, there lived a curious weaver named Marner. He would wake before dawn to tend to his loom, listening to the soft rhythms of the shuttle and the distant songs of the morning birds. Each day he would think about the stories he had heard as a child. Tales of travelers, storms weathered, and friendships that bound the villagers together. On this particular day, Marner decided to walk to the market to see what news had arrived from the neighboring town. He had no idea that a short conversation would change the way he viewed his work and his life.\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"WER\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": null,\n        \"min\": 0.017,\n        \"max\": 0.017,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          0.017\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def benchmark_nemo(model_name: str, audio_path: str, ground_truth: str):\n",
        "    \"\"\"\n",
        "    Benchmark a NeMo ASR model (Parakeet or Conformer).\n",
        "    \"\"\"\n",
        "    # Load model\n",
        "    model = nemo_asr.models.EncDecCTCModelBPE.from_pretrained(model_name=model_name)\n",
        "\n",
        "    # Time inference\n",
        "    start = time.time()\n",
        "    transcript_result = model.transcribe([audio_path])[0]\n",
        "    end = time.time()\n",
        "\n",
        "    # Extract transcript string - handle potential Hypothesis object or string\n",
        "    if isinstance(transcript_result, str):\n",
        "        transcript = transcript_result\n",
        "    elif hasattr(transcript_result, 'text'):\n",
        "        transcript = transcript_result.text\n",
        "    else:\n",
        "        transcript = str(transcript_result) # Fallback to string conversion\n",
        "\n",
        "\n",
        "    latency = end - start\n",
        "    error = wer(ground_truth.lower(), transcript.lower())\n",
        "\n",
        "    return {\n",
        "        \"model\": model_name,\n",
        "        \"latency_sec\": latency,\n",
        "        \"transcript\": transcript,\n",
        "        \"WER\": error\n",
        "    }"
      ],
      "metadata": {
        "id": "TDVBwvhKcphK"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "results = []\n",
        "\n",
        "\n",
        "\n",
        "# Whisper small\n",
        "results.append(benchmark_whisper(audio_path, ground_truth, model_size=\"small\"))\n",
        "\n",
        "# NeMo Parakeet small\n",
        "results.append(benchmark_nemo(\"nvidia/parakeet-tdt_ctc-110m\", audio_path, ground_truth))\n",
        "\n",
        "# NeMo Conformer small\n",
        "results.append(benchmark_nemo(\"stt_en_conformer_ctc_small\", audio_path, ground_truth))\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "f86c12ea502841c18c67cca90f227e6d",
            "f4b7f618b9c04946acc00a43de9b4cc6",
            "154d77a930de4eec83eb68e283a7d101",
            "6905bb2a97dc4fcfb5be75a7f889ea58",
            "08111013e9ea481caa4bd947a6e74da8",
            "0ec9a7df8078460daf24c77c5ff76ed6",
            "fb503ec2a7b24d86b4dbdf386f912296",
            "294baaee8fbf4259a98141145b17ac4a",
            "79c5512508a648dcb20e715745af74e4",
            "4dc1d33750494c099cea5c19737e1a62",
            "2a9315d6899348bba92a41ef98db1d34"
          ]
        },
        "id": "doJ0E1FMfGzh",
        "outputId": "36b6da93-b5cb-467a-e13f-1059c40296b4"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "parakeet-tdt_ctc-110m.nemo:   0%|          | 0.00/459M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "f86c12ea502841c18c67cca90f227e6d"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[NeMo I 2025-09-14 04:51:40 nemo_logging:393] Tokenizer SentencePieceTokenizer initialized with 1024 tokens\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[NeMo W 2025-09-14 04:51:42 nemo_logging:405] If you intend to do training or fine-tuning, please call the ModelPT.setup_training_data() method and provide a valid configuration file to setup the train data loader.\n",
            "    Train config : \n",
            "    manifest_filepath: null\n",
            "    sample_rate: 16000\n",
            "    batch_size: null\n",
            "    shuffle: true\n",
            "    num_workers: 8\n",
            "    pin_memory: true\n",
            "    max_duration: 40\n",
            "    min_duration: 0.1\n",
            "    is_tarred: true\n",
            "    tarred_audio_filepaths: null\n",
            "    shuffle_n: 2048\n",
            "    bucketing_strategy: fully_randomized\n",
            "    bucketing_batch_size: null\n",
            "    shard_manifests: true\n",
            "    use_lhotse: true\n",
            "    use_bucketing: true\n",
            "    num_buckets: 30\n",
            "    bucket_duration_bins: null\n",
            "    batch_duration: 600\n",
            "    defer_setup: true\n",
            "    \n",
            "[NeMo W 2025-09-14 04:51:42 nemo_logging:405] If you intend to do validation, please call the ModelPT.setup_validation_data() or ModelPT.setup_multiple_validation_data() method and provide a valid configuration file to setup the validation data loader(s). \n",
            "    Validation config : \n",
            "    manifest_filepath: null\n",
            "    sample_rate: 16000\n",
            "    batch_size: 32\n",
            "    shuffle: false\n",
            "    use_start_end_token: false\n",
            "    num_workers: 8\n",
            "    pin_memory: true\n",
            "    \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[NeMo I 2025-09-14 04:51:42 nemo_logging:393] PADDING: 0\n",
            "[NeMo I 2025-09-14 04:51:44 nemo_logging:393] Using RNNT Loss : tdt\n",
            "    Loss tdt_kwargs: {'fastemit_lambda': 0.0, 'clamp': -1.0, 'durations': [0, 1, 2, 3, 4], 'sigma': 0.02, 'omega': 0.1}\n",
            "[NeMo I 2025-09-14 04:51:44 nemo_logging:393] Using RNNT Loss : tdt\n",
            "    Loss tdt_kwargs: {'fastemit_lambda': 0.0, 'clamp': -1.0, 'durations': [0, 1, 2, 3, 4], 'sigma': 0.02, 'omega': 0.1}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[NeMo W 2025-09-14 04:51:44 nemo_logging:405] No conditional node support for Cuda.\n",
            "    Cuda graphs with while loops are disabled, decoding speed will be slower\n",
            "    Reason: CUDA is not available\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[NeMo I 2025-09-14 04:51:44 nemo_logging:393] Using RNNT Loss : tdt\n",
            "    Loss tdt_kwargs: {'fastemit_lambda': 0.0, 'clamp': -1.0, 'durations': [0, 1, 2, 3, 4], 'sigma': 0.02, 'omega': 0.1}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[NeMo W 2025-09-14 04:51:44 nemo_logging:405] No conditional node support for Cuda.\n",
            "    Cuda graphs with while loops are disabled, decoding speed will be slower\n",
            "    Reason: CUDA is not available\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[NeMo I 2025-09-14 04:51:45 nemo_logging:393] Model EncDecHybridRNNTCTCBPEModel was successfully restored from /root/.cache/huggingface/hub/models--nvidia--parakeet-tdt_ctc-110m/snapshots/431a349f3051ab85c22b9b7a2741b5fe77065665/parakeet-tdt_ctc-110m.nemo.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Transcribing: 100%|██████████| 1/1 [00:05<00:00,  5.23s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[NeMo I 2025-09-14 04:51:50 nemo_logging:393] Found existing object /root/.cache/torch/NeMo/NeMo_2.4.0/stt_en_conformer_ctc_small/5d2d8e5b2b5adb8f5091363c6ba19c55/stt_en_conformer_ctc_small.nemo.\n",
            "[NeMo I 2025-09-14 04:51:50 nemo_logging:393] Re-using file from: /root/.cache/torch/NeMo/NeMo_2.4.0/stt_en_conformer_ctc_small/5d2d8e5b2b5adb8f5091363c6ba19c55/stt_en_conformer_ctc_small.nemo\n",
            "[NeMo I 2025-09-14 04:51:50 nemo_logging:393] Instantiating model from pre-trained checkpoint\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[NeMo I 2025-09-14 04:51:51 nemo_logging:393] Tokenizer SentencePieceTokenizer initialized with 1024 tokens\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[NeMo W 2025-09-14 04:51:51 nemo_logging:405] If you intend to do training or fine-tuning, please call the ModelPT.setup_training_data() method and provide a valid configuration file to setup the train data loader.\n",
            "    Train config : \n",
            "    manifest_filepath: /data/NeMo_ASR_SET/English/v2.0/train/tarred_audio_manifest.json\n",
            "    sample_rate: 16000\n",
            "    batch_size: 64\n",
            "    shuffle: true\n",
            "    num_workers: 8\n",
            "    pin_memory: true\n",
            "    use_start_end_token: false\n",
            "    trim_silence: false\n",
            "    max_duration: 20.0\n",
            "    min_duration: 0.1\n",
            "    shuffle_n: 2048\n",
            "    is_tarred: true\n",
            "    tarred_audio_filepaths: /data/NeMo_ASR_SET/English/v2.0/train/audio__OP_0..4095_CL_.tar\n",
            "    \n",
            "[NeMo W 2025-09-14 04:51:51 nemo_logging:405] If you intend to do validation, please call the ModelPT.setup_validation_data() or ModelPT.setup_multiple_validation_data() method and provide a valid configuration file to setup the validation data loader(s). \n",
            "    Validation config : \n",
            "    manifest_filepath:\n",
            "    - /data/ASR/LibriSpeech/librispeech_withsp2/manifests/librivox-dev-other.json\n",
            "    - /data/ASR/LibriSpeech/librispeech_withsp2/manifests/librivox-dev-clean.json\n",
            "    - /data/ASR/LibriSpeech/librispeech_withsp2/manifests/librivox-test-other.json\n",
            "    - /data/ASR/LibriSpeech/librispeech_withsp2/manifests/librivox-test-clean.json\n",
            "    sample_rate: 16000\n",
            "    batch_size: 64\n",
            "    shuffle: false\n",
            "    num_workers: 8\n",
            "    pin_memory: true\n",
            "    use_start_end_token: false\n",
            "    is_tarred: false\n",
            "    tarred_audio_filepaths: na\n",
            "    \n",
            "[NeMo W 2025-09-14 04:51:51 nemo_logging:405] Please call the ModelPT.setup_test_data() or ModelPT.setup_multiple_test_data() method and provide a valid configuration file to setup the test data loader(s).\n",
            "    Test config : \n",
            "    manifest_filepath:\n",
            "    - /data/ASR/LibriSpeech/librispeech_withsp2/manifests/librivox-test-other.json\n",
            "    - /data/ASR/LibriSpeech/librispeech_withsp2/manifests/librivox-dev-clean.json\n",
            "    - /data/ASR/LibriSpeech/librispeech_withsp2/manifests/librivox-dev-other.json\n",
            "    - /data/ASR/LibriSpeech/librispeech_withsp2/manifests/librivox-test-clean.json\n",
            "    sample_rate: 16000\n",
            "    batch_size: 64\n",
            "    shuffle: false\n",
            "    num_workers: 8\n",
            "    pin_memory: true\n",
            "    use_start_end_token: false\n",
            "    is_tarred: false\n",
            "    tarred_audio_filepaths: na\n",
            "    \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[NeMo I 2025-09-14 04:51:51 nemo_logging:393] PADDING: 0\n",
            "[NeMo I 2025-09-14 04:51:52 nemo_logging:393] Model EncDecCTCModelBPE was successfully restored from /root/.cache/torch/NeMo/NeMo_2.4.0/stt_en_conformer_ctc_small/5d2d8e5b2b5adb8f5091363c6ba19c55/stt_en_conformer_ctc_small.nemo.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Transcribing: 100%|██████████| 1/1 [00:02<00:00,  2.33s/it]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.DataFrame(results )\n",
        "print(df)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D7-sHImtiiP-",
        "outputId": "f958d0f8-6cde-4275-b5fa-73257fa56b17"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                          model  latency_sec  \\\n",
            "0  nvidia/parakeet-tdt_ctc-110m     5.262635   \n",
            "1    stt_en_conformer_ctc_small     2.348751   \n",
            "\n",
            "                                          transcript       WER  \n",
            "0  Once upon a time, in a quiet village nestled b...  0.026087  \n",
            "1  once upon a time in a quiet village nestled be...  0.130435  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Vygec1cYiisO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "0i56pCf0fS2M"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}